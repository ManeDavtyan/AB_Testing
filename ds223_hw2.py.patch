# Patch generated by Pyment v0.3.3

--- a/ds223_hw2.py
+++ b/ds223_hw2.py
@@ -12,6 +12,7 @@
 
 
 class Bandit(ABC):
+    """ """
 
     def __init__(self, p):
         self.p = p  # Probability of success for the bandit
@@ -23,16 +24,24 @@
 
     # inheritance function, same for both algorithms
     def pull(self):
+        """ """
         return np.random.random() < self.p
 
     def update(self, x):
+        """
+
+        :param x: 
+
+        """
         self.N += 1
         self.p_estimate = ((self.N - 1) * self.p_estimate + x) / self.N
 
     def experiment(self):
+        """ """
         pass
 
     def report(self):
+        """ """
         # store data in csv
         # print average reward (use f strings to make it informative)
         # print average regret (use f strings to make it informative)
@@ -41,6 +50,7 @@
 
 # Thompson Sampling class
 class ThompsonSampling(Bandit):
+    """ """
     def __init__(self, probabilities):
         self.bandits = [Bandit(p) for p in probabilities]  # Create bandits with given success probabilities
 
@@ -48,13 +58,24 @@
         return "Thompson Sampling"
 
     def sample(self):
+        """ """
         return [bandit.pull() for bandit in self.bandits]  # Simulate pulling all bandits
 
     def update(self, results):  # Update bandit success information
+        """
+
+        :param results: 
+
+        """
         for i, result in enumerate(results):
             self.bandits[i].update(result)
 
     def plot(self, trial):  # Plotting the progress by the Time
+        """
+
+        :param trial: 
+
+        """
         x = np.linspace(0, 1, 200)
         for bandit in self.bandits:
             y = beta.pdf(x, bandit.p_estimate * bandit.N, (1 - bandit.p_estimate) * bandit.N)
@@ -65,6 +86,11 @@
         plt.show()
 
     def experiment(self, num_trials):
+        """
+
+        :param num_trials: 
+
+        """
         sample_points = [5, 10, 20, 50, 100, 200, 500, 1000, 1500, num_trials - 1]
         rewards = np.zeros(num_trials)
 
@@ -80,6 +106,11 @@
         return rewards
 
     def report(self, num_trials):
+        """
+
+        :param num_trials: 
+
+        """
         rewards = self.experiment(num_trials)
         cumulative_rewards = np.cumsum(rewards)
         win_rates = cumulative_rewards / (np.arange(num_trials) + 1)
@@ -103,6 +134,7 @@
 
 
 class EpsilonGreedy(Bandit):
+    """ """
     random.seed(42)
 
     def __init__(self, probabilities, epsilon):
@@ -112,15 +144,22 @@
         self.t = 0  # Initialize time step t
 
     def decay_epsilon(self):
+        """ """
         self.epsilon = 1 / (self.t + 1)  # Decay epsilon as time progresses
 
     def choose_bandit(self):
+        """ """
         if np.random.random() < self.epsilon:
             return np.random.randint(len(self.bandits))  # Choose a random bandit for exploration
         else:
             return np.argmax([bandit.p_estimate for bandit in self.bandits])
 
     def experiment(self, num_trials):
+        """
+
+        :param num_trials: 
+
+        """
         sample_points = [5, 10, 20, 50, 100, 200, 500, 1000, 1500, num_trials - 1]
         rewards = np.zeros(num_trials)
 
@@ -138,6 +177,11 @@
         return rewards
 
     def report(self, num_trials):
+        """
+
+        :param num_trials: 
+
+        """
         rewards = self.experiment(num_trials)
         cumulative_rewards = np.cumsum(rewards)
         win_rates = cumulative_rewards / (np.arange(num_trials) + 1)
@@ -155,6 +199,11 @@
                 csv_writer.writerow([i + 1, rewards[i], cumulative_rewards[i], win_rates[i], actual_reward])
 
     def plot(self, trial):
+        """
+
+        :param trial: 
+
+        """
         x = np.linspace(0, 1, 200)
         for bandit in self.bandits:
             y = beta.pdf(x, bandit.p_estimate * bandit.N, (1 - bandit.p_estimate) * bandit.N)
@@ -171,6 +220,7 @@
 
 
 def comparison():
+    """ """
     # Create instances of the two algorithms
     thompson_sampling = ThompsonSampling(BANDIT_PROBABILITIES)
     epsilon_greedy = EpsilonGreedy(BANDIT_PROBABILITIES, EPS)
@@ -191,7 +241,14 @@
 
 
 class Visualization():
+    """ """
     def plot1(self, thompson_bandits, epsilon_bandits):
+        """
+
+        :param thompson_bandits: 
+        :param epsilon_bandits: 
+
+        """
         # Visualize the performance of each bandit: linear and log
         for i, (t_bandit, e_bandit) in enumerate(zip(thompson_bandits, epsilon_bandits)):
             x = np.arange(1, t_bandit.N + 1)
@@ -218,6 +275,12 @@
             plt.show()
 
     def plot2(self, thompson_rewards, epsilon_rewards):
+        """
+
+        :param thompson_rewards: 
+        :param epsilon_rewards: 
+
+        """
         # Compare E-greedy and Thompson Sampling cumulative rewards and cumulative regrets
         x = np.arange(len(thompson_rewards))
         thompson_cumulative_rewards = np.cumsum(thompson_rewards)
